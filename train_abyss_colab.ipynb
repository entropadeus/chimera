{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chimera Abyss Training - A100 Beast Mode\n",
    "\n",
    "**~2.1B Parameter Model | 96 Layers Deep | Pure Madness**\n",
    "\n",
    "Training data mix:\n",
    "- Chat/Conversation (OpenAssistant, Dolly, UltraChat)\n",
    "- Creative Writing (WritingPrompts, TinyStories)\n",
    "- Eloquent Prose (Project Gutenberg)\n",
    "- General Knowledge (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers sentencepiece accelerate datasets tqdm matplotlib\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/chimera_abyss'\n",
    "DATA_DIR = '/content/drive/MyDrive/chimera_data'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"Data: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Chimera Files\n",
    "\n",
    "Upload `model.py` and `tokenizer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload manually\n",
    "from google.colab import files\n",
    "print(\"Upload model.py and tokenizer.py:\")\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Copy from Drive\n",
    "CHIMERA_DRIVE_PATH = '/content/drive/MyDrive/chimera'\n",
    "\n",
    "import shutil\n",
    "if os.path.exists(CHIMERA_DRIVE_PATH):\n",
    "    for f in ['model.py', 'tokenizer.py']:\n",
    "        src = os.path.join(CHIMERA_DRIVE_PATH, f)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, '/content/')\n",
    "            print(f\"Copied {f}\")\n",
    "else:\n",
    "    print(f\"Path not found: {CHIMERA_DRIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists('/content/model.py'), \"model.py not found!\"\n",
    "assert os.path.exists('/content/tokenizer.py'), \"tokenizer.py not found!\"\n",
    "print(\"Core files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download & Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    name: str\n",
    "    hf_path: str\n",
    "    hf_split: str = \"train\"\n",
    "    hf_subset: Optional[str] = None\n",
    "    text_field: str = \"text\"\n",
    "    max_samples: Optional[int] = None\n",
    "    min_length: int = 100\n",
    "    max_length: int = 8000\n",
    "    category: str = \"general\"\n",
    "    weight: float = 1.0\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    \"oasst2\": DatasetConfig(\n",
    "        name=\"OpenAssistant OASST2\",\n",
    "        hf_path=\"OpenAssistant/oasst2\",\n",
    "        text_field=\"text\",\n",
    "        max_samples=50000,\n",
    "        min_length=50,\n",
    "        category=\"chat\",\n",
    "        weight=1.5,\n",
    "    ),\n",
    "    \"dolly\": DatasetConfig(\n",
    "        name=\"Databricks Dolly 15k\",\n",
    "        hf_path=\"databricks/databricks-dolly-15k\",\n",
    "        text_field=\"instruction,response\",\n",
    "        max_samples=15000,\n",
    "        min_length=50,\n",
    "        category=\"chat\",\n",
    "        weight=1.2,\n",
    "    ),\n",
    "    \"ultrachat\": DatasetConfig(\n",
    "        name=\"UltraChat\",\n",
    "        hf_path=\"stingning/ultrachat\",\n",
    "        text_field=\"data\",\n",
    "        max_samples=80000,\n",
    "        min_length=100,\n",
    "        category=\"chat\",\n",
    "        weight=1.3,\n",
    "    ),\n",
    "    \"writing_prompts\": DatasetConfig(\n",
    "        name=\"WritingPrompts\",\n",
    "        hf_path=\"euclaise/writingprompts\",\n",
    "        text_field=\"story\",\n",
    "        max_samples=40000,\n",
    "        min_length=200,\n",
    "        max_length=10000,\n",
    "        category=\"creative\",\n",
    "        weight=1.5,\n",
    "    ),\n",
    "    \"tinystories\": DatasetConfig(\n",
    "        name=\"TinyStories\",\n",
    "        hf_path=\"roneneldan/TinyStories\",\n",
    "        text_field=\"text\",\n",
    "        max_samples=80000,\n",
    "        min_length=100,\n",
    "        category=\"creative\",\n",
    "        weight=1.0,\n",
    "    ),\n",
    "    \"gutenberg\": DatasetConfig(\n",
    "        name=\"Project Gutenberg\",\n",
    "        hf_path=\"sedthh/gutenberg_english\",\n",
    "        text_field=\"TEXT\",\n",
    "        max_samples=3000,\n",
    "        min_length=500,\n",
    "        max_length=15000,\n",
    "        category=\"eloquent\",\n",
    "        weight=2.0,\n",
    "    ),\n",
    "    \"wikipedia\": DatasetConfig(\n",
    "        name=\"Wikipedia\",\n",
    "        hf_path=\"wikipedia\",\n",
    "        hf_subset=\"20220301.simple\",\n",
    "        text_field=\"text\",\n",
    "        max_samples=40000,\n",
    "        min_length=200,\n",
    "        max_length=5000,\n",
    "        category=\"general\",\n",
    "        weight=0.8,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(DATASETS)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://[^\\s]+', '', text)\n",
    "    # Remove reddit artifacts\n",
    "    text = text.replace('[removed]', '').replace('[deleted]', '')\n",
    "    # Normalize quotes\n",
    "    for old, new in [('\"', '\"'), ('\"', '\"'), (\"'\", \"'\"), (\"'\", \"'\")]:\n",
    "        text = text.replace(old, new)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def is_quality_text(text, min_len, max_len):\n",
    "    if not text or len(text) < min_len or len(text) > max_len:\n",
    "        return False\n",
    "    words = text.split()\n",
    "    if len(words) < 20:\n",
    "        return False\n",
    "    if len(set(words)) / len(words) < 0.3:\n",
    "        return False\n",
    "    alpha_count = sum(1 for c in text if c.isalpha())\n",
    "    if alpha_count / len(text) < 0.6:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def format_conversation(messages):\n",
    "    formatted = []\n",
    "    for i, msg in enumerate(messages):\n",
    "        if isinstance(msg, dict):\n",
    "            role = msg.get('role', msg.get('from', 'user'))\n",
    "            content = msg.get('content', msg.get('value', msg.get('text', '')))\n",
    "        else:\n",
    "            role = 'user' if i % 2 == 0 else 'assistant'\n",
    "            content = str(msg)\n",
    "        \n",
    "        if role in ['user', 'human', 'prompter']:\n",
    "            formatted.append(f\"Human: {content}\")\n",
    "        elif role in ['assistant', 'gpt', 'bot']:\n",
    "            formatted.append(f\"Assistant: {content}\")\n",
    "        else:\n",
    "            formatted.append(content)\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(name, config):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {config.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    try:\n",
    "        if config.hf_subset:\n",
    "            dataset = load_dataset(config.hf_path, config.hf_subset, \n",
    "                                   split=config.hf_split, streaming=True)\n",
    "        else:\n",
    "            dataset = load_dataset(config.hf_path, split=config.hf_split, \n",
    "                                   streaming=True, trust_remote_code=True)\n",
    "        \n",
    "        count = 0\n",
    "        pbar = tqdm(dataset, desc=config.name, total=config.max_samples)\n",
    "        \n",
    "        for item in pbar:\n",
    "            if config.max_samples and count >= config.max_samples:\n",
    "                break\n",
    "            \n",
    "            text = \"\"\n",
    "            \n",
    "            if 'oasst' in name:\n",
    "                text = item.get('text', '')\n",
    "                role = item.get('role', 'user')\n",
    "                if role == 'prompter':\n",
    "                    text = f\"Human: {text}\"\n",
    "                else:\n",
    "                    text = f\"Assistant: {text}\"\n",
    "                    \n",
    "            elif 'dolly' in name:\n",
    "                inst = item.get('instruction', '')\n",
    "                ctx = item.get('context', '')\n",
    "                resp = item.get('response', '')\n",
    "                if ctx:\n",
    "                    text = f\"Human: {inst}\\n\\nContext: {ctx}\\n\\nAssistant: {resp}\"\n",
    "                else:\n",
    "                    text = f\"Human: {inst}\\n\\nAssistant: {resp}\"\n",
    "                    \n",
    "            elif 'ultrachat' in name:\n",
    "                data = item.get('data', [])\n",
    "                if isinstance(data, list) and len(data) >= 2:\n",
    "                    text = format_conversation(data)\n",
    "                    \n",
    "            elif 'writing_prompts' in name:\n",
    "                prompt = item.get('prompt', '')\n",
    "                story = item.get('story', '')\n",
    "                # Clean WP tags\n",
    "                prompt = re.sub(r'^\\s*\\[WP\\]|\\[OT\\]|\\[EU\\]', '', prompt).strip()\n",
    "                if story:\n",
    "                    text = f\"Prompt: {prompt}\\n\\n{story}\" if prompt else story\n",
    "                    \n",
    "            else:\n",
    "                if ',' in config.text_field:\n",
    "                    fields = [f.strip() for f in config.text_field.split(',')]\n",
    "                    parts = [str(item.get(f, '')) for f in fields if item.get(f)]\n",
    "                    text = '\\n\\n'.join(parts)\n",
    "                else:\n",
    "                    text = item.get(config.text_field, '')\n",
    "            \n",
    "            text = clean_text(text)\n",
    "            if is_quality_text(text, config.min_length, config.max_length):\n",
    "                texts.append(text)\n",
    "                count += 1\n",
    "                pbar.set_postfix({'kept': count})\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"Collected {len(texts):,} samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "print(\"Dataset processor ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT DATASETS (comment out ones you don't want)\n",
    "DATASETS_TO_USE = [\n",
    "    \"oasst2\",\n",
    "    \"dolly\",\n",
    "    \"ultrachat\",\n",
    "    \"writing_prompts\",\n",
    "    \"tinystories\",\n",
    "    \"gutenberg\",\n",
    "    \"wikipedia\",\n",
    "]\n",
    "\n",
    "print(f\"Will process {len(DATASETS_TO_USE)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(DATA_DIR, 'abyss_train.txt')\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    size_gb = os.path.getsize(DATA_PATH) / 1e9\n",
    "    print(f\"Data exists: {DATA_PATH} ({size_gb:.2f} GB)\")\n",
    "    print(\"Delete to regenerate, or skip to next section.\")\n",
    "    SKIP_DATA_PREP = True\n",
    "else:\n",
    "    print(\"Will download and process data.\")\n",
    "    SKIP_DATA_PREP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD AND PROCESS (takes 15-30 min)\n",
    "if not SKIP_DATA_PREP:\n",
    "    all_texts = []\n",
    "    category_counts = defaultdict(int)\n",
    "    \n",
    "    for name in DATASETS_TO_USE:\n",
    "        cfg = DATASETS[name]\n",
    "        texts = process_dataset(name, cfg)\n",
    "        \n",
    "        if cfg.weight > 1.0:\n",
    "            texts = texts * int(cfg.weight)\n",
    "        \n",
    "        all_texts.extend(texts)\n",
    "        category_counts[cfg.category] += len(texts)\n",
    "    \n",
    "    print(f\"\\nTotal: {len(all_texts):,}\")\n",
    "    \n",
    "    random.seed(42)\n",
    "    random.shuffle(all_texts)\n",
    "    \n",
    "    # Dedupe\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for t in tqdm(all_texts, desc=\"Dedup\"):\n",
    "        h = hashlib.md5(t[:500].encode()).hexdigest()\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            unique.append(t)\n",
    "    print(f\"Removed {len(all_texts) - len(unique):,} dupes\")\n",
    "    all_texts = unique\n",
    "    \n",
    "    with open(DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "        for t in tqdm(all_texts, desc=\"Writing\"):\n",
    "            f.write(t.strip() + '\\n\\n')\n",
    "    \n",
    "    print(f\"\\nSaved: {DATA_PATH}\")\n",
    "    print(f\"Size: {os.path.getsize(DATA_PATH)/1e9:.2f} GB\")\n",
    "    for cat, cnt in sorted(category_counts.items()):\n",
    "        print(f\"  {cat}: {cnt:,}\")\n",
    "else:\n",
    "    print(\"Skipping (data exists)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class StepChecker:\n",
    "    def __init__(self, max_steps, log_every=10, plot_every=100):\n",
    "        self.max_steps = max_steps\n",
    "        self.log_every = log_every\n",
    "        self.plot_every = plot_every\n",
    "        self.steps, self.losses, self.ppls = [], [], []\n",
    "        self.gpu_mem, self.tps = [], []\n",
    "        self.start_time = None\n",
    "        self.step_times = deque(maxlen=100)\n",
    "        self.last_time = None\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_step = 0\n",
    "        \n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "        self.last_time = self.start_time\n",
    "        print(\"=\"*70)\n",
    "        print(\"CHIMERA ABYSS - STEP CHECKER\")\n",
    "        print(f\"Max Steps: {self.max_steps:,}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    def log(self, step, loss, ppl, lr, tokens):\n",
    "        now = time.time()\n",
    "        dt = now - self.last_time\n",
    "        self.step_times.append(dt)\n",
    "        self.last_time = now\n",
    "        \n",
    "        gpu = torch.cuda.memory_allocated() / 1e9\n",
    "        self.steps.append(step)\n",
    "        self.losses.append(loss)\n",
    "        self.ppls.append(min(ppl, 1000))\n",
    "        self.gpu_mem.append(gpu)\n",
    "        \n",
    "        t = tokens / dt if dt > 0 else 0\n",
    "        self.tps.append(t)\n",
    "        \n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.best_step = step\n",
    "        \n",
    "        avg_dt = sum(self.step_times) / len(self.step_times)\n",
    "        eta = timedelta(seconds=int((self.max_steps - step) * avg_dt))\n",
    "        pct = step / self.max_steps\n",
    "        bar = '#' * int(40 * pct) + '-' * (40 - int(40 * pct))\n",
    "        \n",
    "        msg = f\"[{bar}] {pct*100:.1f}% | Step {step:,} | Loss {loss:.4f} | PPL {ppl:.2f} | GPU {gpu:.1f}GB | {t:.0f} tok/s | ETA {eta}\"\n",
    "        print(f\"\\r{msg}\", end=\"\")\n",
    "        \n",
    "        if step % self.log_every == 0:\n",
    "            print()\n",
    "        if step % self.plot_every == 0 and step > 0:\n",
    "            self.plot()\n",
    "            \n",
    "    def plot(self):\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        ax[0,0].plot(self.steps, self.losses, 'b-', alpha=0.7)\n",
    "        ax[0,0].axhline(self.best_loss, color='g', linestyle='--', label=f'Best: {self.best_loss:.4f}')\n",
    "        ax[0,0].set_title('Loss'); ax[0,0].legend(); ax[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        ax[0,1].plot(self.steps, self.ppls, 'r-', alpha=0.7)\n",
    "        ax[0,1].set_title('Perplexity'); ax[0,1].set_yscale('log'); ax[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        ax[1,0].plot(self.steps, self.gpu_mem, 'g-', alpha=0.7)\n",
    "        ax[1,0].axhline(80, color='r', linestyle='--', label='A100 80GB')\n",
    "        ax[1,0].set_title('VRAM (GB)'); ax[1,0].set_ylim(0, 85); ax[1,0].legend(); ax[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        ax[1,1].plot(self.steps, self.tps, 'm-', alpha=0.7)\n",
    "        ax[1,1].set_title('Throughput (tok/s)'); ax[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout(); plt.show()\n",
    "        elapsed = timedelta(seconds=int(time.time() - self.start_time))\n",
    "        print(f\"Elapsed: {elapsed} | Best: {self.best_loss:.4f} @ {self.best_step}\")\n",
    "        \n",
    "    def finish(self):\n",
    "        elapsed = timedelta(seconds=int(time.time() - self.start_time))\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"COMPLETE | Time: {elapsed} | Final: {self.losses[-1]:.4f} | Best: {self.best_loss:.4f}\")\n",
    "        print(\"=\"*70)\n",
    "        self.plot()\n",
    "        \n",
    "    def save_history(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({'steps': self.steps, 'losses': self.losses, 'best': self.best_loss}, f)\n",
    "\n",
    "\n",
    "print(\"StepChecker ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_config: str = \"abyss\"\n",
    "    data_path: str = DATA_PATH\n",
    "    seq_length: int = 1024\n",
    "    micro_batch_size: int = 6\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    learning_rate: float = 2e-4\n",
    "    min_lr: float = 1e-5\n",
    "    weight_decay: float = 0.1\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95\n",
    "    grad_clip: float = 1.0\n",
    "    warmup_steps: int = 500\n",
    "    max_steps: int = 15000\n",
    "    mixed_precision: bool = True\n",
    "    compile_model: bool = True\n",
    "    output_dir: str = CHECKPOINT_DIR\n",
    "    save_every: int = 500\n",
    "    log_every: int = 10\n",
    "    plot_every: int = 200\n",
    "    generate_every: int = 500\n",
    "    resume_from: Optional[str] = None\n",
    "\n",
    "\n",
    "config = TrainConfig()\n",
    "print(\"Config ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content')\n",
    "\n",
    "from model import Chimera, chimera_abyss\n",
    "from tokenizer import ChimeraTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda')\n",
    "tokenizer = ChimeraTokenizer()\n",
    "print(f\"Vocab: {tokenizer.vocab_size}\")\n",
    "\n",
    "model_cfg = chimera_abyss()\n",
    "model_cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"Layers: {model_cfg.n_layers}, d_model: {model_cfg.d_model}\")\n",
    "\n",
    "model = Chimera(model_cfg).to(device)\n",
    "print(f\"Params: {model.get_num_params():,} ({model.get_num_params()/1e9:.2f}B)\")\n",
    "\n",
    "if config.compile_model:\n",
    "    print(\"Compiling...\")\n",
    "    model = torch.compile(model)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "print(f\"VRAM: {torch.cuda.memory_allocated()/1e9:.1f}GB / 80GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedDataset(IterableDataset):\n",
    "    def __init__(self, data_path, tokenizer, seq_length=512):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        with open(self.data_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                tokens = self.tokenizer.encode(line, add_bos=True, add_eos=True)\n",
    "                buffer.extend(tokens)\n",
    "                while len(buffer) >= self.seq_length + 1:\n",
    "                    seq = buffer[:self.seq_length + 1]\n",
    "                    yield {\"input_ids\": torch.tensor(seq[:-1]), \"labels\": torch.tensor(seq[1:])}\n",
    "                    buffer = buffer[self.seq_length:]\n",
    "\n",
    "\n",
    "train_dataset = PackedDataset(config.data_path, tokenizer, config.seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.micro_batch_size, num_workers=2, pin_memory=True)\n",
    "print(\"DataLoader ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, cfg):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if \"norm\" in n or \"bias\" in n or \"embedding\" in n:\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return torch.optim.AdamW(\n",
    "        [{\"params\": decay, \"weight_decay\": cfg.weight_decay},\n",
    "         {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
    "        lr=cfg.learning_rate, betas=(cfg.beta1, cfg.beta2), fused=True)\n",
    "\n",
    "\n",
    "def get_cosine_schedule(opt, warmup, max_steps, min_ratio=0.1):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup:\n",
    "            return step / max(1, warmup)\n",
    "        progress = (step - warmup) / max(1, max_steps - warmup)\n",
    "        return max(min_ratio, 0.5 * (1 + math.cos(math.pi * progress)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "\n",
    "optimizer = create_optimizer(model, config)\n",
    "scheduler = get_cosine_schedule(optimizer, config.warmup_steps, config.max_steps, config.min_lr/config.learning_rate)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "print(\"Optimizer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(model, tokenizer, prompt, max_tokens=100, temperature=0.8):\n",
    "    model.eval()\n",
    "    ids = torch.tensor([tokenizer.encode(prompt, add_bos=True)], device=device)\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            logits, _ = model(ids)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "        ids = torch.cat([ids, next_tok], 1)\n",
    "        if next_tok.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    model.train()\n",
    "    return tokenizer.decode(ids[0].tolist())\n",
    "\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Human: What is the meaning of life?\\n\\nAssistant:\",\n",
    "    \"Once upon a time, in a kingdom far away,\",\n",
    "    \"The ancient tome spoke of a prophecy:\",\n",
    "    \"Human: Write me a poem about the stars.\\n\\nAssistant:\",\n",
    "]\n",
    "print(\"Generation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 0\n",
    "latest_path = os.path.join(config.output_dir, 'latest.pt')\n",
    "\n",
    "if config.resume_from and os.path.exists(config.resume_from):\n",
    "    ckpt = torch.load(config.resume_from, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    scheduler.load_state_dict(ckpt['scheduler'])\n",
    "    if 'scaler' in ckpt:\n",
    "        scaler.load_state_dict(ckpt['scaler'])\n",
    "    start_step = ckpt['step']\n",
    "    print(f\"Resumed from {start_step}\")\n",
    "elif os.path.exists(latest_path):\n",
    "    ckpt = torch.load(latest_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    scheduler.load_state_dict(ckpt['scheduler'])\n",
    "    if 'scaler' in ckpt:\n",
    "        scaler.load_state_dict(ckpt['scaler'])\n",
    "    start_step = ckpt['step']\n",
    "    print(f\"Resumed from latest: {start_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_checker = StepChecker(config.max_steps, config.log_every, config.plot_every)\n",
    "step_checker.start()\n",
    "\n",
    "model.train()\n",
    "data_iter = iter(train_loader)\n",
    "step = start_step\n",
    "micro_step = 0\n",
    "running_loss = 0.0\n",
    "running_tokens = 0\n",
    "\n",
    "try:\n",
    "    while step < config.max_steps:\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(train_loader)\n",
    "            batch = next(data_iter)\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            logits, _ = model(input_ids)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        running_loss += loss.item() * config.gradient_accumulation_steps\n",
    "        running_tokens += input_ids.numel()\n",
    "        micro_step += 1\n",
    "        \n",
    "        if micro_step % config.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "            \n",
    "            avg_loss = running_loss / config.gradient_accumulation_steps\n",
    "            ppl = math.exp(min(avg_loss, 20))\n",
    "            step_checker.log(step, avg_loss, ppl, scheduler.get_last_lr()[0], running_tokens)\n",
    "            running_loss = 0\n",
    "            running_tokens = 0\n",
    "            \n",
    "            if step % config.save_every == 0:\n",
    "                ckpt = {'step': step, 'model': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "                        'scheduler': scheduler.state_dict(), 'scaler': scaler.state_dict()}\n",
    "                torch.save(ckpt, os.path.join(config.output_dir, f'step_{step}.pt'))\n",
    "                torch.save(ckpt, os.path.join(config.output_dir, 'latest.pt'))\n",
    "                print(f\"\\n[Saved step_{step}.pt]\")\n",
    "            \n",
    "            if step % config.generate_every == 0:\n",
    "                print(f\"\\n{'='*60}\\nSAMPLES @ {step}\\n{'='*60}\")\n",
    "                for p in TEST_PROMPTS:\n",
    "                    out = generate_sample(model, tokenizer, p, 100)\n",
    "                    print(f\"\\n> {p[:50]}...\\n{out}\")\n",
    "                print('='*60)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrupted! Saving...\")\n",
    "    ckpt = {'step': step, 'model': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(), 'scaler': scaler.state_dict()}\n",
    "    torch.save(ckpt, os.path.join(config.output_dir, f'interrupted_{step}.pt'))\n",
    "    torch.save(ckpt, os.path.join(config.output_dir, 'latest.pt'))\n",
    "\n",
    "step_checker.finish()\n",
    "step_checker.save_history(os.path.join(config.output_dir, 'history.json'))\n",
    "torch.save(model.state_dict(), os.path.join(config.output_dir, 'model_final.pt'))\n",
    "print(f\"\\nSaved: {config.output_dir}/model_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chat with Chimera (type 'quit' to exit):\\n\")\n",
    "while True:\n",
    "    user = input(\"You: \")\n",
    "    if user.lower() == 'quit':\n",
    "        break\n",
    "    prompt = f\"Human: {user}\\n\\nAssistant:\"\n",
    "    response = generate_sample(model, tokenizer, prompt, 200, 0.8)\n",
    "    if \"Assistant:\" in response:\n",
    "        response = response.split(\"Assistant:\")[-1].strip()\n",
    "    print(f\"\\nChimera: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = os.path.join(config.output_dir, 'chimera_abyss_weights.pt')\n",
    "torch.save(model.state_dict(), export_path)\n",
    "print(f\"Exported: {export_path}\")\n",
    "print(f\"Size: {os.path.getsize(export_path)/1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
