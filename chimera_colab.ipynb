{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêâ Chimera Medium (350M) - Colab Training\n",
    "\n",
    "**IMPORTANT: Run cells in order! Don't skip any.**\n",
    "\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n",
    "2. Run ALL cells from top to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup - Check GPU & Install Dependencies { display-mode: \"form\" }\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check GPU\n",
    "print(\"=\" * 50)\n",
    "print(\"CHECKING GPU...\")\n",
    "print(\"=\" * 50)\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n‚ùå ERROR: No GPU detected!\")\n",
    "    print(\"Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "    raise SystemExit(\"No GPU\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INSTALLING DEPENDENCIES...\")\n",
    "print(\"=\" * 50)\n",
    "!pip install -q transformers datasets sentencepiece\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Upload Code - Upload chimera_code.zip { display-mode: \"form\" }\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/content/chimera/data', exist_ok=True)\n",
    "os.makedirs('/content/chimera/checkpoints', exist_ok=True)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"UPLOAD chimera_code.zip (44KB)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if not uploaded:\n",
    "    print(\"‚ùå No file uploaded!\")\n",
    "    raise SystemExit(\"Upload failed\")\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"Extracting {filename}...\")\n",
    "    with zipfile.ZipFile(filename, 'r') as z:\n",
    "        z.extractall('/content/chimera')\n",
    "\n",
    "%cd /content/chimera\n",
    "print(\"\\n‚úÖ Code uploaded! Files:\")\n",
    "!ls -la *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Download ALL Training Data { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "os.chdir('/content/chimera')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DOWNLOADING TINYSTORIES (this takes a few minutes)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download TinyStories\n",
    "print(\"Downloading from HuggingFace...\")\n",
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "print(f\"Loaded {len(ds):,} stories\")\n",
    "\n",
    "# Save to text file\n",
    "print(\"\\nSaving to data/tinystories.txt...\")\n",
    "with open('data/tinystories.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, item in enumerate(ds):\n",
    "        f.write(item['text'].strip() + '\\n\\n')\n",
    "        if (i + 1) % 500000 == 0:\n",
    "            print(f\"  Written {i+1:,} stories...\")\n",
    "\n",
    "print(\"\\n‚úÖ TinyStories downloaded!\")\n",
    "!ls -lh data/tinystories.txt\n",
    "\n",
    "# Download OASST for fine-tuning later\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DOWNLOADING OASST CONVERSATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python -u download_oasst.py --output data/oasst_data.jsonl --max-examples 10000\n",
    "\n",
    "# Verify all data exists\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"VERIFYING DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists('data/tinystories.txt'):\n",
    "    size = os.path.getsize('data/tinystories.txt') / (1024*1024)\n",
    "    print(f\"‚úÖ tinystories.txt: {size:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå tinystories.txt MISSING!\")\n",
    "    raise SystemExit(\"Data missing\")\n",
    "\n",
    "if os.path.exists('data/oasst_data.jsonl'):\n",
    "    size = os.path.getsize('data/oasst_data.jsonl') / (1024*1024)\n",
    "    print(f\"‚úÖ oasst_data.jsonl: {size:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è oasst_data.jsonl missing (fine-tuning won't work)\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL DATA READY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Verify Model Can Load { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "os.chdir('/content/chimera')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TESTING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from model import Chimera, chimera_medium\n",
    "from tokenizer import ChimeraTokenizer\n",
    "import torch\n",
    "\n",
    "config = chimera_medium()\n",
    "tokenizer = ChimeraTokenizer()\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = Chimera(config)\n",
    "params = model.get_num_params()\n",
    "\n",
    "print(f\"\\n‚úÖ Chimera Medium\")\n",
    "print(f\"   Parameters: {params:,} ({params/1e6:.0f}M)\")\n",
    "print(f\"   d_model: {config.d_model}\")\n",
    "print(f\"   layers: {config.n_layers}\")\n",
    "print(f\"   vocab: {config.vocab_size:,}\")\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ MODEL READY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 5. üöÄ START PRETRAINING (45-60 min on A100) { display-mode: \"form\" }\n\nimport os\nos.chdir('/content/chimera')\n\n# Final check before training\nif not os.path.exists('data/tinystories.txt'):\n    print(\"‚ùå ERROR: Training data not found!\")\n    print(\"Run cell 3 first to download data.\")\n    raise SystemExit(\"No data\")\n\nprint(\"=\" * 50)\nprint(\"STARTING PRETRAINING\")\nprint(\"Model: Chimera Medium (350M params)\")\nprint(\"Target: Loss < 2.0, PPL < 7\")\nprint(\"Time: ~45-60 min on A100\")\nprint(\"=\" * 50)\nprint(\"\\nTip: Save checkpoints to Drive periodically!\\n\")\n\n!python -u train_packed.py \\\n    --model-config medium \\\n    --max-steps 3000 \\\n    --micro-batch-size 16 \\\n    --gradient-accumulation-steps 2 \\\n    --lr 3e-4 \\\n    --save-every 500 \\\n    --compile"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 6. Extract Model Weights { display-mode: \"form\" }\n\nimport os\nos.chdir('/content/chimera')\nimport torch\n\nprint(\"=\" * 50)\nprint(\"EXTRACTING MODEL WEIGHTS\")\nprint(\"=\" * 50)\n\n# Find latest checkpoint\nif os.path.exists('checkpoints/latest.pt'):\n    ckpt_path = 'checkpoints/latest.pt'\nelse:\n    # Find any checkpoint\n    ckpts = [f for f in os.listdir('checkpoints') if f.startswith('step_')]\n    if ckpts:\n        ckpt_path = f'checkpoints/{sorted(ckpts)[-1]}'\n    else:\n        print(\"‚ùå No checkpoints found! Run pretraining first.\")\n        raise SystemExit(\"No checkpoint\")\n\nprint(f\"Loading {ckpt_path}...\")\nckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n\nprint(f\"Step: {ckpt.get('step', '?')}\")\nprint(f\"Loss: {ckpt.get('loss', '?')}\")\n\n# Save just model weights\ntorch.save(ckpt['model'], 'checkpoints/medium_pretrained.pt')\nprint(\"\\n‚úÖ Saved to checkpoints/medium_pretrained.pt\")\n!ls -lh checkpoints/medium_pretrained.pt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Test Generation { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "os.chdir('/content/chimera')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TESTING GENERATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python generate.py \\\n",
    "    --checkpoint checkpoints/medium_pretrained.pt \\\n",
    "    --model-config medium \\\n",
    "    --prompt \"Once upon a time, there was a little rabbit\" \\\n",
    "    --max-tokens 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 8. Fine-tune for Chat (15-20 min on A100) { display-mode: \"form\" }\n\nimport os\nos.chdir('/content/chimera')\n\nprint(\"=\" * 50)\nprint(\"GENERATING CONVERSATION DATASET\")\nprint(\"=\" * 50)\n\n!python -u create_instruct_data.py \\\n    --input data/tinystories.txt \\\n    --output data/instruct_data.jsonl \\\n    --max-stories 5000 \\\n    --external data/oasst_data.jsonl \\\n    --external-format oasst \\\n    --max-external 10000\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"FINE-TUNING FOR CONVERSATION\")\nprint(\"=\" * 50)\n\n!python -u train_instruct.py \\\n    --model-path checkpoints/medium_pretrained.pt \\\n    --model-config medium \\\n    --data-path data/instruct_data.jsonl \\\n    --batch-size 8 \\\n    --gradient-accumulation-steps 2 \\\n    --epochs 2 \\\n    --compile"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Test Chat { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "os.chdir('/content/chimera')\n",
    "\n",
    "print(\"Testing conversational responses...\\n\")\n",
    "\n",
    "!python generate.py \\\n",
    "    --checkpoint checkpoints/instruct_final.pt \\\n",
    "    --model-config medium \\\n",
    "    --prompt \"Hello! What is your name?\" \\\n",
    "    --max-tokens 100\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "!python generate.py \\\n",
    "    --checkpoint checkpoints/instruct_final.pt \\\n",
    "    --model-config medium \\\n",
    "    --prompt \"Tell me a short story about a brave dog.\" \\\n",
    "    --max-tokens 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Save to Google Drive { display-mode: \"form\" }\n",
    "\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "os.chdir('/content/chimera')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SAVING TO GOOGLE DRIVE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create chimera folder in Drive\n",
    "drive_path = '/content/drive/MyDrive/chimera_models'\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "# Copy models\n",
    "files_to_save = [\n",
    "    'checkpoints/medium_pretrained.pt',\n",
    "    'checkpoints/instruct_final.pt',\n",
    "    'checkpoints/instruct_best.pt',\n",
    "    'checkpoints/latest.pt'\n",
    "]\n",
    "\n",
    "for f in files_to_save:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"Copying {f}...\")\n",
    "        shutil.copy(f, drive_path)\n",
    "        print(f\"  ‚úÖ Saved!\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {f} not found, skipping\")\n",
    "\n",
    "print(\"\\n‚úÖ MODELS SAVED TO GOOGLE DRIVE!\")\n",
    "print(f\"Location: {drive_path}\")\n",
    "!ls -lh {drive_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11. Direct Download (Alternative) { display-mode: \"form\" }\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "os.chdir('/content/chimera')\n",
    "\n",
    "print(\"Downloading models directly to your computer...\")\n",
    "print(\"(This may be slow for large files)\\n\")\n",
    "\n",
    "if os.path.exists('checkpoints/medium_pretrained.pt'):\n",
    "    print(\"Downloading medium_pretrained.pt...\")\n",
    "    files.download('checkpoints/medium_pretrained.pt')\n",
    "\n",
    "if os.path.exists('checkpoints/instruct_final.pt'):\n",
    "    print(\"Downloading instruct_final.pt...\")\n",
    "    files.download('checkpoints/instruct_final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Resume Training (if disconnected)\n",
    "\n",
    "If Colab disconnects:\n",
    "1. Re-run cells 1-4\n",
    "2. Run the resume cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Resume from Google Drive Checkpoint { display-mode: \"form\" }\n\nimport os\nos.chdir('/content/chimera')\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy checkpoint from Drive\ndrive_ckpt = '/content/drive/MyDrive/chimera_models/latest.pt'\nif os.path.exists(drive_ckpt):\n    print(\"Copying checkpoint from Drive...\")\n    !cp \"{drive_ckpt}\" checkpoints/latest.pt\n    print(\"‚úÖ Checkpoint restored!\")\nelse:\n    print(\"‚ùå No checkpoint found in Drive\")\n    print(\"Starting fresh...\")\n\n# Resume training (A100 settings)\nif os.path.exists('checkpoints/latest.pt'):\n    print(\"\\nResuming training...\")\n    !python -u train_packed.py \\\n        --model-config medium \\\n        --resume checkpoints/latest.pt \\\n        --max-steps 3000 \\\n        --micro-batch-size 16 \\\n        --gradient-accumulation-steps 2 \\\n        --save-every 500 \\\n        --compile\nelse:\n    print(\"No checkpoint to resume from. Run cell 5 to start fresh.\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}