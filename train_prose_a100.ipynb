{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêâ Chimera Prose - 500M Eloquent Writing Model\n",
    "\n",
    "**Target**: ~500M parameter hybrid recurrent-attention LLM for creative writing\n",
    "\n",
    "**Hardware**: A100 40GB (Colab Pro+)\n",
    "\n",
    "**Architecture**: Chimera (Griffin-style RG-LRU + Sliding Window GQA)\n",
    "\n",
    "**Training tricks from successful models**:\n",
    "- ŒºP-inspired scaling (Cerebras/Microsoft)\n",
    "- Cosine LR with warmup (GPT/Llama)\n",
    "- AdamW Œ≤2=0.95 (Llama-2)\n",
    "- Gradient checkpointing\n",
    "- bf16 mixed precision\n",
    "- torch.compile (PyTorch 2.0+)\n",
    "- Packed sequences (no padding waste)\n",
    "- Quality-filtered data blend\n",
    "\n",
    "**Data blend**:\n",
    "- 40% Literary prose (pg19/BookCorpus)\n",
    "- 30% Creative writing (WritingPrompts)\n",
    "- 20% Conversational (OpenAssistant)\n",
    "- 10% High-quality web (Dolma/FineWeb subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup & Dependencies { display-mode: \"form\" }\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate wandb sentencepiece\n",
    "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available, using standard attention\"\n",
    "\n",
    "# Clone repo\n",
    "if not os.path.exists('chimera'):\n",
    "    !git clone https://github.com/entropadeus/chimera.git\n",
    "os.chdir('chimera')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Setup complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Model Configuration - 500M Prose Model { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ProseModelConfig:\n",
    "    \"\"\"\n",
    "    ~500M params optimized for A100 40GB with headroom.\n",
    "    \n",
    "    Architecture choices:\n",
    "    - d_model=1536: Sweet spot for expressiveness vs memory\n",
    "    - 24 layers: Good depth for style/voice development\n",
    "    - 24 heads: 64 head_dim (standard, hardware efficient)\n",
    "    - 6 KV heads: 4:1 GQA ratio (memory efficient)\n",
    "    - 2048 context: Enough for prose, fits in memory\n",
    "    - 1024 sliding window: Local coherence\n",
    "    \n",
    "    Memory estimate:\n",
    "    - Model: ~1GB (bf16)\n",
    "    - Optimizer states: ~4GB (AdamW)\n",
    "    - Activations: ~8-12GB (with grad checkpointing)\n",
    "    - Gradients: ~1GB\n",
    "    - Total: ~15-20GB, leaving ~20GB headroom\n",
    "    \"\"\"\n",
    "    d_model: int = 1536\n",
    "    n_layers: int = 24\n",
    "    n_heads: int = 24          # 1536/24 = 64 head_dim\n",
    "    n_kv_heads: int = 6        # 4:1 GQA\n",
    "    vocab_size: int = 32000\n",
    "    max_seq_len: int = 2048\n",
    "    sliding_window: int = 1024\n",
    "    ffn_hidden_mult: float = 8/3  # SwiGLU ratio\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    rope_scaling: Optional[float] = None\n",
    "    dropout: float = 0.0       # No dropout (modern practice)\n",
    "    attention_every_n: int = 3  # 3:1 recurrent:attention\n",
    "    tie_word_embeddings: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.recurrence_dim = self.d_model\n",
    "        self.ffn_hidden = int(self.d_model * self.ffn_hidden_mult)\n",
    "        self.ffn_hidden = ((self.ffn_hidden + 255) // 256) * 256\n",
    "        self.head_dim = self.d_model // self.n_heads\n",
    "\n",
    "config = ProseModelConfig()\n",
    "\n",
    "# Calculate param count\n",
    "def estimate_params(cfg):\n",
    "    embed = cfg.vocab_size * cfg.d_model\n",
    "    \n",
    "    n_attn = cfg.n_layers // cfg.attention_every_n\n",
    "    n_recur = cfg.n_layers - n_attn\n",
    "    \n",
    "    # Attention layer params\n",
    "    attn_qkv = cfg.d_model * (cfg.n_heads + 2 * cfg.n_kv_heads) * cfg.head_dim\n",
    "    attn_out = cfg.n_heads * cfg.head_dim * cfg.d_model\n",
    "    attn_total = (attn_qkv + attn_out) * n_attn\n",
    "    \n",
    "    # Recurrent layer params (with input gate)\n",
    "    recur_input = cfg.d_model * cfg.d_model  # input_proj\n",
    "    recur_gates = 2 * cfg.d_model * cfg.d_model  # input_gate + recurrence_gate\n",
    "    recur_lambda = cfg.d_model  # lambda_param\n",
    "    recur_out = cfg.d_model * cfg.d_model  # output_proj\n",
    "    recur_total = (recur_input + recur_gates + recur_lambda + recur_out) * n_recur\n",
    "    \n",
    "    # FFN per layer\n",
    "    ffn = 3 * cfg.d_model * cfg.ffn_hidden * cfg.n_layers\n",
    "    \n",
    "    # Norms\n",
    "    norms = 2 * cfg.d_model * cfg.n_layers + cfg.d_model\n",
    "    \n",
    "    total = embed + attn_total + recur_total + ffn + norms\n",
    "    return total\n",
    "\n",
    "params = estimate_params(config)\n",
    "print(f\"Estimated parameters: {params:,} ({params/1e6:.1f}M)\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Layers: {config.n_layers} ({config.n_layers - config.n_layers//config.attention_every_n} recurrent, {config.n_layers//config.attention_every_n} attention)\")\n",
    "print(f\"  d_model: {config.d_model}\")\n",
    "print(f\"  Heads: {config.n_heads} query, {config.n_kv_heads} KV (GQA)\")\n",
    "print(f\"  Context: {config.max_seq_len} tokens\")\n",
    "print(f\"  FFN hidden: {config.ffn_hidden}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Download & Prepare High-Quality Training Data { display-mode: \"form\" }\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport random\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\"\\nDownloading datasets (this takes a few minutes)...\")\n\n# 1. Literary prose - Project Gutenberg (use HF parquet version)\nprint(\"  [1/4] Loading literary prose...\")\ntry:\n    # Try the working pg19 parquet version\n    pg19 = load_dataset(\"emozilla/pg19\", split=\"train\", streaming=True)\n    pg19_texts = []\n    for i, x in enumerate(pg19):\n        if i >= 3000:\n            break\n        if x.get('text') and len(x['text']) > 1000:\n            pg19_texts.append(x['text'][:50000])\n    print(f\"    Loaded {len(pg19_texts)} literary texts\")\nexcept Exception as e:\n    print(f\"    pg19 failed: {e}\")\n    # Fallback: use a public domain books dataset\n    try:\n        books = load_dataset(\"storytracer/US_PD_Books\", split=\"train\", streaming=True)\n        pg19_texts = []\n        for i, x in enumerate(books):\n            if i >= 3000:\n                break\n            if x.get('text') and len(x['text']) > 1000:\n                pg19_texts.append(x['text'][:50000])\n        print(f\"    Loaded {len(pg19_texts)} book texts (fallback)\")\n    except:\n        print(\"    Using tinystories as prose fallback...\")\n        ts = load_dataset(\"roneneldan/TinyStories\", split=\"train[:50000]\")\n        pg19_texts = [x['text'] for x in ts if len(x['text']) > 200]\n\n# 2. Creative writing - WritingPrompts\nprint(\"  [2/4] Loading creative fiction...\")\ntry:\n    wp = load_dataset(\"Lambent/writing-prompts-cleaned\", split=\"train\", streaming=True)\n    wp_texts = []\n    for i, x in enumerate(wp):\n        if i >= 30000:\n            break\n        story = x.get('story') or x.get('text', '')\n        prompt = x.get('prompt', '')\n        if story and len(story) > 300:\n            wp_texts.append(f\"Prompt: {prompt}\\n\\nStory: {story}\" if prompt else story)\n    print(f\"    Loaded {len(wp_texts)} creative texts\")\nexcept Exception as e:\n    print(f\"    WritingPrompts failed: {e}\")\n    try:\n        # Alternative creative writing dataset\n        wp = load_dataset(\"lksy/prompts_stories\", split=\"train\", streaming=True)\n        wp_texts = []\n        for i, x in enumerate(wp):\n            if i >= 30000:\n                break\n            if x.get('story') and len(x['story']) > 300:\n                wp_texts.append(x['story'])\n        print(f\"    Loaded {len(wp_texts)} stories (fallback)\")\n    except:\n        wp_texts = []\n        print(\"    No creative writing dataset available\")\n\n# 3. Conversational - OpenAssistant\nprint(\"  [3/4] Loading conversational data...\")\ntry:\n    oasst = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n    oasst_texts = []\n    for x in oasst:\n        if x.get('role') == 'assistant' and x.get('text') and len(x['text']) > 200:\n            oasst_texts.append(x['text'])\n    oasst_texts = oasst_texts[:20000]\n    print(f\"    Loaded {len(oasst_texts)} conversational texts\")\nexcept Exception as e:\n    print(f\"    OASST failed: {e}\")\n    oasst_texts = []\n\n# 4. High-quality web text - FineWeb-Edu\nprint(\"  [4/4] Loading quality web text...\")\ntry:\n    fineweb = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\", streaming=True)\n    fineweb_texts = []\n    for i, x in enumerate(fineweb):\n        if i >= 15000:\n            break\n        if x.get('text') and len(x['text']) > 500:\n            fineweb_texts.append(x['text'][:10000])\n    print(f\"    Loaded {len(fineweb_texts)} web texts\")\nexcept Exception as e:\n    print(f\"    FineWeb failed: {e}\")\n    try:\n        # Fallback to SlimPajama\n        slim = load_dataset(\"cerebras/SlimPajama-627B\", split=\"train\", streaming=True)\n        fineweb_texts = []\n        for i, x in enumerate(slim):\n            if i >= 15000:\n                break\n            if x.get('text') and len(x['text']) > 500:\n                fineweb_texts.append(x['text'][:10000])\n        print(f\"    Loaded {len(fineweb_texts)} texts (SlimPajama fallback)\")\n    except:\n        fineweb_texts = []\n        print(\"    No web text available\")\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Literary: {len(pg19_texts):,} texts\")\nprint(f\"  Creative: {len(wp_texts):,} texts\")\nprint(f\"  Conversational: {len(oasst_texts):,} texts\")\nprint(f\"  Web: {len(fineweb_texts):,} texts\")\n\n# Blend with target ratios: 40% literary, 30% creative, 20% conversational, 10% web\nall_texts = []\n\nif pg19_texts:\n    n_lit = min(40000, len(pg19_texts))\n    all_texts.extend(random.sample(pg19_texts, n_lit) if len(pg19_texts) > n_lit else pg19_texts)\n    \nif wp_texts:\n    n_creative = min(30000, len(wp_texts))\n    all_texts.extend(random.sample(wp_texts, n_creative) if len(wp_texts) > n_creative else wp_texts)\n    \nif oasst_texts:\n    n_conv = min(20000, len(oasst_texts))\n    all_texts.extend(random.sample(oasst_texts, n_conv) if len(oasst_texts) > n_conv else oasst_texts)\n    \nif fineweb_texts:\n    n_web = min(10000, len(fineweb_texts))\n    all_texts.extend(random.sample(fineweb_texts, n_web) if len(fineweb_texts) > n_web else fineweb_texts)\n\nrandom.shuffle(all_texts)\n\nprint(f\"\\nFinal blend: {len(all_texts):,} texts\")\n\nif len(all_texts) < 1000:\n    raise ValueError(\"Not enough training data! Check dataset availability.\")\n\n# Save to disk\nos.makedirs('data', exist_ok=True)\nwith open('data/prose_blend.txt', 'w', encoding='utf-8') as f:\n    f.write('\\n\\n<|endoftext|>\\n\\n'.join(all_texts))\n\nfile_size = os.path.getsize('data/prose_blend.txt') / 1e9\nprint(f\"Saved to data/prose_blend.txt ({file_size:.2f} GB)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Tokenize & Pack Sequences { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "SEQ_LEN = 2048  # Match model config\n",
    "\n",
    "print(\"Tokenizing corpus...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "with open('data/prose_blend.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize entire corpus\n",
    "print(f\"Text length: {len(text):,} chars\")\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(f\"Token count: {len(tokens):,}\")\n",
    "\n",
    "# Pack into sequences (no padding waste)\n",
    "n_seqs = len(tokens) // SEQ_LEN\n",
    "tokens = tokens[:n_seqs * SEQ_LEN]\n",
    "packed = np.array(tokens, dtype=np.int32).reshape(n_seqs, SEQ_LEN)\n",
    "\n",
    "print(f\"\\nPacked sequences: {n_seqs:,} x {SEQ_LEN}\")\n",
    "print(f\"Total tokens: {n_seqs * SEQ_LEN:,}\")\n",
    "\n",
    "# Save\n",
    "np.save('data/prose_packed.npy', packed)\n",
    "print(f\"Saved to data/prose_packed.npy\")\n",
    "\n",
    "# Quick stats\n",
    "print(f\"\\nDataset size: {packed.nbytes / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Training Configuration { display-mode: \"form\" }\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    \"\"\"\n",
    "    Training configuration with tricks from successful models.\n",
    "    \n",
    "    Key insights:\n",
    "    - Llama-2: AdamW Œ≤2=0.95, weight_decay=0.1, grad_clip=1.0\n",
    "    - GPT-3: Cosine LR decay, warmup 0.1% of steps\n",
    "    - Chinchilla: ~20 tokens per parameter optimal\n",
    "    - ŒºP: Scale LR inversely with width for transfer\n",
    "    \"\"\"\n",
    "    # Batch size (tune for A100 40GB)\n",
    "    micro_batch_size: int = 4         # Per-GPU batch\n",
    "    gradient_accumulation: int = 16   # Effective batch = 4 * 16 = 64\n",
    "    \n",
    "    # Learning rate (ŒºP-inspired: scale down for wider models)\n",
    "    # Base LR 3e-4 for 768 width, scale as sqrt(768/width)\n",
    "    base_lr: float = 2e-4             # Slightly lower for 1536 width\n",
    "    min_lr: float = 2e-5              # 10% of base\n",
    "    warmup_steps: int = 1000          # ~2% of training\n",
    "    \n",
    "    # Optimizer (Llama-2 style)\n",
    "    weight_decay: float = 0.1\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95               # Lower than default for stability\n",
    "    eps: float = 1e-8\n",
    "    grad_clip: float = 1.0\n",
    "    \n",
    "    # Training duration\n",
    "    max_steps: int = 50000            # ~3.2B tokens with batch 64 * 2048\n",
    "    eval_interval: int = 500\n",
    "    save_interval: int = 2500\n",
    "    log_interval: int = 10\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing: bool = True\n",
    "    mixed_precision: str = \"bf16\"     # A100 native bf16\n",
    "    compile_model: bool = True        # torch.compile speedup\n",
    "    \n",
    "    # Data\n",
    "    seq_len: int = 2048\n",
    "    \n",
    "    # Paths\n",
    "    data_path: str = \"data/prose_packed.npy\"\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    \n",
    "    # Wandb\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"chimera-prose\"\n",
    "    wandb_run_name: str = \"prose-500m-a100\"\n",
    "\n",
    "train_config = TrainConfig()\n",
    "\n",
    "# Calculate training stats\n",
    "effective_batch = train_config.micro_batch_size * train_config.gradient_accumulation\n",
    "tokens_per_step = effective_batch * train_config.seq_len\n",
    "total_tokens = train_config.max_steps * tokens_per_step\n",
    "\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Effective batch size: {effective_batch}\")\n",
    "print(f\"Tokens per step: {tokens_per_step:,}\")\n",
    "print(f\"Total training tokens: {total_tokens:,} ({total_tokens/1e9:.1f}B)\")\n",
    "print(f\"\\nLearning rate: {train_config.base_lr} ‚Üí {train_config.min_lr}\")\n",
    "print(f\"Warmup steps: {train_config.warmup_steps}\")\n",
    "print(f\"\\nOptimizations:\")\n",
    "print(f\"  Gradient checkpointing: {train_config.gradient_checkpointing}\")\n",
    "print(f\"  Mixed precision: {train_config.mixed_precision}\")\n",
    "print(f\"  torch.compile: {train_config.compile_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Build Model with Optimizations { display-mode: \"form\" }\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import Chimera, ChimeraConfig\n",
    "\n",
    "# Build config\n",
    "model_config = ChimeraConfig(\n",
    "    d_model=1536,\n",
    "    n_layers=24,\n",
    "    n_heads=24,\n",
    "    n_kv_heads=6,\n",
    "    vocab_size=32000,\n",
    "    max_seq_len=2048,\n",
    "    sliding_window=1024,\n",
    "    attention_every_n=3,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = Chimera(model_config)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if train_config.gradient_checkpointing:\n",
    "    # Wrap each layer with checkpointing\n",
    "    from torch.utils.checkpoint import checkpoint\n",
    "    \n",
    "    class CheckpointedChimeraBlock(nn.Module):\n",
    "        def __init__(self, block):\n",
    "            super().__init__()\n",
    "            self.block = block\n",
    "            self.use_attention = block.use_attention\n",
    "        \n",
    "        def forward(self, x, cache=None, position_offset=0, use_cache=False):\n",
    "            if self.training and not use_cache:\n",
    "                # Use checkpointing during training\n",
    "                def custom_forward(x_inner):\n",
    "                    return self.block(x_inner, None, position_offset, False)[0]\n",
    "                return checkpoint(custom_forward, x, use_reentrant=False), None\n",
    "            return self.block(x, cache, position_offset, use_cache)\n",
    "    \n",
    "    model.layers = nn.ModuleList([\n",
    "        CheckpointedChimeraBlock(layer) for layer in model.layers\n",
    "    ])\n",
    "    print(\"  ‚úì Gradient checkpointing enabled\")\n",
    "\n",
    "# Compile model (PyTorch 2.0+)\n",
    "if train_config.compile_model and hasattr(torch, 'compile'):\n",
    "    print(\"  Compiling model (takes ~2 min first run)...\")\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "    print(\"  ‚úì torch.compile enabled\")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {n_params:,} ({n_params/1e6:.1f}M)\")\n",
    "print(f\"  Trainable parameters: {n_trainable:,}\")\n",
    "print(f\"  Model size (bf16): {n_params * 2 / 1e9:.2f} GB\")\n",
    "\n",
    "# Memory estimate\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(f\"  GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Setup Optimizer & Scheduler { display-mode: \"form\" }\n",
    "\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Separate weight decay for different param types (Llama-2 style)\n",
    "def get_param_groups(model, weight_decay):\n",
    "    \"\"\"Apply weight decay only to weights, not biases/norms/embeddings.\"\"\"\n",
    "    decay = set()\n",
    "    no_decay = set()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # No decay for biases, norms, embeddings\n",
    "        if 'bias' in name or 'norm' in name or 'embed' in name or 'lambda' in name:\n",
    "            no_decay.add(name)\n",
    "        else:\n",
    "            decay.add(name)\n",
    "    \n",
    "    param_dict = {name: param for name, param in model.named_parameters()}\n",
    "    \n",
    "    return [\n",
    "        {\"params\": [param_dict[n] for n in sorted(decay)], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [param_dict[n] for n in sorted(no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "param_groups = get_param_groups(model, train_config.weight_decay)\n",
    "print(f\"Param groups: {len(param_groups[0]['params'])} with decay, {len(param_groups[1]['params'])} without\")\n",
    "\n",
    "# AdamW optimizer (Llama-2 betas)\n",
    "optimizer = AdamW(\n",
    "    param_groups,\n",
    "    lr=train_config.base_lr,\n",
    "    betas=(train_config.beta1, train_config.beta2),\n",
    "    eps=train_config.eps,\n",
    ")\n",
    "\n",
    "# Cosine schedule with warmup\n",
    "def get_lr_lambda(step):\n",
    "    \"\"\"Cosine decay with linear warmup.\"\"\"\n",
    "    if step < train_config.warmup_steps:\n",
    "        # Linear warmup\n",
    "        return step / train_config.warmup_steps\n",
    "    else:\n",
    "        # Cosine decay to min_lr\n",
    "        progress = (step - train_config.warmup_steps) / (train_config.max_steps - train_config.warmup_steps)\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        # Decay from base_lr to min_lr\n",
    "        lr_range = 1.0 - (train_config.min_lr / train_config.base_lr)\n",
    "        return (train_config.min_lr / train_config.base_lr) + lr_range * cosine_decay\n",
    "\n",
    "scheduler = LambdaLR(optimizer, get_lr_lambda)\n",
    "\n",
    "# Mixed precision scaler (bf16 doesn't need scaling on A100, but we set it up anyway)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(train_config.mixed_precision == \"fp16\"))\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"  Base LR: {train_config.base_lr}\")\n",
    "print(f\"  Min LR: {train_config.min_lr}\")\n",
    "print(f\"  Weight decay: {train_config.weight_decay}\")\n",
    "print(f\"  Betas: ({train_config.beta1}, {train_config.beta2})\")\n",
    "print(f\"\\nScheduler: Cosine with {train_config.warmup_steps} warmup steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Data Loader { display-mode: \"form\" }\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PackedDataset(Dataset):\n",
    "    \"\"\"Dataset for pre-packed token sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data = np.load(data_path)\n",
    "        print(f\"Loaded {len(self.data):,} sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = torch.from_numpy(self.data[idx].astype(np.int64))\n",
    "        # Input: all but last token, Target: all but first token\n",
    "        return tokens[:-1], tokens[1:]\n",
    "\n",
    "dataset = PackedDataset(train_config.data_path)\n",
    "\n",
    "# DataLoader with shuffling\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=train_config.micro_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader:\")\n",
    "print(f\"  Batch size: {train_config.micro_batch_size}\")\n",
    "print(f\"  Batches per epoch: {len(dataloader):,}\")\n",
    "print(f\"  Tokens per epoch: {len(dataset) * train_config.seq_len:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Training Loop { display-mode: \"form\" }\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize wandb\n",
    "if train_config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=train_config.wandb_project,\n",
    "        name=train_config.wandb_run_name,\n",
    "        config={\n",
    "            \"model\": {\n",
    "                \"d_model\": model_config.d_model,\n",
    "                \"n_layers\": model_config.n_layers,\n",
    "                \"n_heads\": model_config.n_heads,\n",
    "                \"params\": n_params,\n",
    "            },\n",
    "            \"training\": vars(train_config),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create checkpoint dir\n",
    "os.makedirs(train_config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Training state\n",
    "global_step = 0\n",
    "tokens_seen = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Precision context\n",
    "autocast_dtype = torch.bfloat16 if train_config.mixed_precision == \"bf16\" else torch.float16\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "pbar = tqdm(total=train_config.max_steps, desc=\"Training\")\n",
    "\n",
    "accumulation_loss = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "while global_step < train_config.max_steps:\n",
    "    # Accumulate gradients\n",
    "    for micro_step in range(train_config.gradient_accumulation):\n",
    "        try:\n",
    "            inputs, targets = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            inputs, targets = next(data_iter)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast(dtype=autocast_dtype):\n",
    "            logits, _ = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.view(-1, model_config.vocab_size),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-100,\n",
    "            )\n",
    "            loss = loss / train_config.gradient_accumulation\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        accumulation_loss += loss.item()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.grad_clip)\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Update counters\n",
    "    global_step += 1\n",
    "    tokens_seen += train_config.micro_batch_size * train_config.gradient_accumulation * train_config.seq_len\n",
    "    \n",
    "    # Logging\n",
    "    if global_step % train_config.log_interval == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        tokens_per_sec = tokens_seen / elapsed\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{accumulation_loss:.4f}',\n",
    "            'ppl': f'{math.exp(accumulation_loss):.2f}',\n",
    "            'lr': f'{current_lr:.2e}',\n",
    "            'tok/s': f'{tokens_per_sec/1000:.1f}k',\n",
    "        })\n",
    "        \n",
    "        if train_config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"train/loss\": accumulation_loss,\n",
    "                \"train/perplexity\": math.exp(accumulation_loss),\n",
    "                \"train/lr\": current_lr,\n",
    "                \"train/grad_norm\": grad_norm.item(),\n",
    "                \"train/tokens_seen\": tokens_seen,\n",
    "                \"train/tokens_per_sec\": tokens_per_sec,\n",
    "            }, step=global_step)\n",
    "        \n",
    "        accumulation_loss = 0.0\n",
    "    \n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if global_step % train_config.save_interval == 0:\n",
    "        checkpoint = {\n",
    "            'step': global_step,\n",
    "            'tokens_seen': tokens_seen,\n",
    "            'model': model.state_dict() if not hasattr(model, '_orig_mod') else model._orig_mod.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'config': vars(model_config),\n",
    "            'train_config': vars(train_config),\n",
    "        }\n",
    "        \n",
    "        ckpt_path = f\"{train_config.checkpoint_dir}/step_{global_step}.pt\"\n",
    "        torch.save(checkpoint, ckpt_path)\n",
    "        print(f\"\\nüíæ Saved checkpoint: {ckpt_path}\")\n",
    "        \n",
    "        # Also save as latest\n",
    "        torch.save(checkpoint, f\"{train_config.checkpoint_dir}/latest.pt\")\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total steps: {global_step:,}\")\n",
    "print(f\"Total tokens: {tokens_seen:,}\")\n",
    "print(f\"Time: {(time.time() - start_time)/3600:.2f} hours\")\n",
    "\n",
    "if train_config.use_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Extract Model Weights { display-mode: \"form\" }\n",
    "\n",
    "print(\"Extracting model weights for inference...\")\n",
    "\n",
    "# Load latest checkpoint\n",
    "ckpt = torch.load(f\"{train_config.checkpoint_dir}/latest.pt\", map_location='cpu')\n",
    "\n",
    "# Save just the model weights\n",
    "model_weights = ckpt['model']\n",
    "torch.save(model_weights, f\"{train_config.checkpoint_dir}/prose_500m.pt\")\n",
    "\n",
    "print(f\"‚úì Saved model weights to {train_config.checkpoint_dir}/prose_500m.pt\")\n",
    "print(f\"  Size: {os.path.getsize(f'{train_config.checkpoint_dir}/prose_500m.pt') / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11. Test Generation { display-mode: \"form\" }\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(prompt, max_new_tokens=200, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated = input_ids\n",
    "    cache = None\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            if cache is None:\n",
    "                logits, cache = model(generated, use_cache=True)\n",
    "            else:\n",
    "                logits, cache = model(generated[:, -1:], cache=cache, \n",
    "                                     position_offset=generated.size(1)-1, use_cache=True)\n",
    "        \n",
    "        # Sample next token\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-p sampling\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The old lighthouse keeper had seen many storms, but none quite like\",\n",
    "    \"In the depths of the ancient library, she discovered a book that\",\n",
    "    \"The city had changed since I'd been away. The streets were\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATION SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    output = generate(prompt, max_new_tokens=150, temperature=0.8)\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 12. Save to Google Drive (Optional) { display-mode: \"form\" }\n",
    "\n",
    "save_to_drive = True  #@param {type:\"boolean\"}\n",
    "\n",
    "if save_to_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    drive_path = \"/content/drive/MyDrive/chimera_checkpoints\"\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    \n",
    "    # Copy model weights\n",
    "    !cp {train_config.checkpoint_dir}/prose_500m.pt {drive_path}/\n",
    "    !cp {train_config.checkpoint_dir}/latest.pt {drive_path}/\n",
    "    \n",
    "    print(f\"‚úì Saved checkpoints to Google Drive: {drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tricks Summary\n",
    "\n",
    "This notebook incorporates lessons from:\n",
    "\n",
    "### Architecture (Griffin + Improvements)\n",
    "- **Dual-gate RG-LRU**: Input gate + recurrence gate (not just one)\n",
    "- **Learned base rates**: `a = œÉ(Œõ)^(c¬∑r_t)` per dimension\n",
    "- **Log-space computation**: Numerical stability\n",
    "- **3:1 recurrent:attention ratio**: Proven in Griffin/Jamba\n",
    "\n",
    "### Optimization (Llama-2 Style)\n",
    "- **AdamW Œ≤2=0.95**: More stable than 0.999\n",
    "- **Weight decay 0.1**: Only on non-embedding weights\n",
    "- **Gradient clipping 1.0**: Prevents explosions\n",
    "- **Cosine LR with warmup**: Smooth convergence\n",
    "\n",
    "### Efficiency (A100 Optimized)\n",
    "- **bf16 mixed precision**: Native A100 support\n",
    "- **Gradient checkpointing**: 3x memory savings\n",
    "- **torch.compile**: 20-40% speedup\n",
    "- **Packed sequences**: No padding waste\n",
    "\n",
    "### Data (Quality Focus)\n",
    "- **40% literary prose**: pg19/BookCorpus\n",
    "- **30% creative fiction**: WritingPrompts\n",
    "- **20% conversational**: OpenAssistant\n",
    "- **10% quality web**: FineWeb-Edu\n",
    "\n",
    "### Next Steps\n",
    "1. Run full 50k steps (~6-8 hours on A100)\n",
    "2. Monitor loss curve in wandb\n",
    "3. Instruction fine-tune with `train_instruct.py`\n",
    "4. Deploy with `chat_ui.py`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}